"""
Comprehensive help text and explanations for Faster Whisper GUI options.
Provides tooltips and detailed explanations for all features.
"""

# Quick tooltips for hover help
TOOLTIPS = {
    "task": "Choose 'Transcribe' to keep the original language, or 'Translate' to convert to English.",
    "model": "Larger models are more accurate but slower. 'turbo' is fastest, 'large-v2' is balanced.",
    "language": "Select the language spoken in the audio, or 'Auto-detect' to let the program identify it.",
    "output_formats": "Choose one or more output formats. SRT and VTT are subtitle formats, TXT is plain text, JSON contains all metadata.",
    "vad_enable": "Voice Activity Detection filters out parts of audio without speech, improving accuracy.",
    "vad_method": "Different VAD methods vary in speed and accuracy. Click ? for details.",
    "word_timestamps": "Include precise timestamps for each word (required for karaoke/highlight words).",
    "highlight_words": "Underline each word as it's spoken in subtitle files (karaoke effect).",
    "diarize_enable": "Identify different speakers in the audio. Essential for interviews, meetings, and podcasts.",
    "diarize_method": "Different methods for identifying speakers. Click ? for detailed comparison.",
    "num_speakers": "Set if you know the exact number of speakers (improves accuracy). Leave empty for auto-detection.",
    "min_speakers": "Minimum number of speakers to detect. Leave empty for no minimum.",
    "max_speakers": "Maximum number of speakers to detect. Leave empty for no maximum.",
    "speaker_label": "Prefix used for speaker identification (e.g., 'SPEAKER', 'Person', 'Host').",
    "diarize_device": "Device to use for diarization. GPU/CUDA is much faster than CPU.",
    "diarize_threads": "Number of CPU threads for diarization. Auto uses optimal setting.",
    "diarize_only": "Perform only speaker identification without transcription. Click ? for details.",
    "return_embeddings": "Save speaker embeddings to files. Click ? for details.",
    "diarize_ff": "Control when diarization is applied relative to audio filters. Click ? for details.",
    "diarize_dump": "Save diarization debug output to files. Click ? for details.",
    "ff_speechnorm": "Amplify quiet speech to make it more audible. Click ? for details.",
    "ff_loudnorm": "Normalize audio volume to EBU R128 standard (broadcast quality). Click ? for details.",
    "ff_lowhighpass": "Remove frequencies outside 50Hz-7800Hz range (focuses on speech). Click ? for details.",
    "ff_tempo": "Adjust playback speed. Values below 1.0 slow down, above 1.0 speed up.",
    "ff_fftdn": "Reduce background noise. Higher values = more aggressive noise reduction.",
    "temperature": "Controls randomness in transcription. 0 = deterministic (recommended), higher = more variation.",
    "beam_size": "Number of transcription candidates to consider. Higher = more accurate but slower (default: 5).",
    "patience": "How long to wait before finalizing a segment. Higher = more patient (default: 2.0).",
    "device": "Device for transcription. CUDA/GPU is much faster if available.",
    "compute_type": "Quantization type for model. Controls how numbers are stored and processed. 'auto' is recommended.",
    "standard_preset": "Apply standard subtitle formatting preset. Click ? for details.",
    "max_line_width": "Maximum characters per subtitle line. Disabled when Standard Preset is enabled. Click ? for details.",
    "max_line_count": "Maximum lines per subtitle segment. Disabled when Standard Preset is enabled. Click ? for details.",
    "max_comma_percentage": "Percentage of line width before breaking at commas. Disabled when Standard Preset is enabled. Click ? for details.",
    "sentence_mode": "Split subtitles at sentence boundaries. Auto-enabled with diarization. Click ? for details.",
    "batch_recursive": "Process files recursively in subdirectories. Click ? for details.",
    "check_files": "Verify input files before processing. Click ? for details.",
}

# Detailed explanations for question mark buttons
DETAILED_HELP = {
    "vad_method": {
        "title": "Voice Activity Detection (VAD) Methods",
        "content": """Voice Activity Detection identifies parts of audio that contain speech, filtering out silence and noise.

Available methods:
‚Ä¢ pyannote_v3: Most accurate, best for complex audio with background noise. Slower but produces best results.
‚Ä¢ silero_v4_fw / silero_v5_fw: Faster, good for most cases. Recommended default for balanced speed and accuracy.
‚Ä¢ silero_v3: Older version, less accurate than v4/v5.
‚Ä¢ webrtc: Very fast and lightweight, but less accurate. Good for real-time applications.
‚Ä¢ auditok: Alternative method, may work better in some specific scenarios.

Recommendation: Start with silero_v4_fw or silero_v5_fw. Use pyannote_v3 if you need maximum accuracy."""
    },
    
    "diarize_method": {
        "title": "Speaker Diarization Methods - Accuracy Guide",
        "content": """Speaker diarization identifies who is speaking at different times in the audio.

Available methods (ranked by accuracy):
‚Ä¢ pyannote_v3.1: ‚≠ê BEST ACCURACY - Latest version, most accurate speaker identification. Recommended default for maximum accuracy. Best for interviews, meetings, podcasts with multiple speakers. Use this when accuracy is your top priority.

‚Ä¢ pyannote_v3.0: ‚≠ê HIGH ACCURACY - Older but stable version, slightly less accurate than v3.1. Use if v3.1 has compatibility issues or if you need a proven stable option.

‚Ä¢ reverb_v2: ‚≠ê GOOD FOR ECHO - Improved reverb method, excellent for challenging audio environments with echo or reverb (recorded in large rooms, halls, auditoriums). Use when your audio has significant reverb or echo that affects speaker separation.

‚Ä¢ reverb_v1: ‚ö†Ô∏è OLDER METHOD - Older reverb method, less accurate than v2. Only use if v2 is not available.

ACCURACY TIPS:
1. Use pyannote_v3.1 for best results (default in Diarize preset)
2. Set exact speaker count if known (dramatically improves accuracy)
3. Use larger models (large-v2, large-v3-turbo) for better diarization
4. Enable audio filters to clean audio first (improves diarization accuracy)
5. Use GPU/CUDA for faster processing (doesn't affect accuracy but makes it practical)

When to use diarization:
‚Ä¢ Interviews and conversations
‚Ä¢ Meetings with multiple participants
‚Ä¢ Podcasts with multiple hosts
‚Ä¢ Any audio where identifying who is speaking is important

Performance: Diarization is computationally intensive. Use GPU/CUDA if available for much faster processing."""
    },
    
    "model": {
        "title": "Whisper Models - Complete Guide",
        "content": """Different models offer different balances of accuracy and speed. Choose based on your priorities.

MODELS RANKED BY ACCURACY (Best to Fastest):

‚≠ê BEST ACCURACY:
‚Ä¢ large-v2: ‚≠ê RECOMMENDED - Excellent accuracy, proven and reliable. Fewer hallucinations than v3 models. Best choice for most use cases, especially when accuracy and reliability are important.

‚Ä¢ large-v3-turbo: Latest turbo-optimized model. Good accuracy with better speed than large-v3. May have more hallucinations than large-v2 in some cases.

‚Ä¢ large-v3: Latest full Whisper model. May produce worse results than large-v2 due to increased hallucinations, especially in difficult audio conditions. Use with caution.

‚Ä¢ large-v1: Older large model. Still accurate but superseded by v2/v3.

BALANCED OPTIONS:
‚Ä¢ medium: Good balance of accuracy and speed. Recommended minimum for decent quality transcriptions. Good for general use.

FAST BUT LESS ACCURATE:
‚Ä¢ small: Faster processing, acceptable accuracy for simple audio. Good for quick transcriptions.

‚Ä¢ base: Base model, faster but less accurate. Use for speed when accuracy is less critical.

‚Ä¢ tiny: Smallest, fastest model. Least accurate. Only use for very fast processing when accuracy can be sacrificed.

LEGACY:
‚Ä¢ turbo: Legacy turbo model (may be same as large-v3-turbo). Fast with good accuracy.

RECOMMENDATIONS:
‚Ä¢ Maximum Accuracy: large-v2 (most reliable, fewer hallucinations) or large-v3-turbo
‚Ä¢ Best Balance: large-v2 or large-v3-turbo
‚Ä¢ Speed Priority: medium or small
‚Ä¢ Quick Processing: base or tiny

Note: large-v3 may produce worse results than large-v2 due to increased hallucinations. large-v2 is recommended for most use cases.

For Diarization: Use large-v2 (recommended) or large-v3-turbo for best speaker identification accuracy.

Note: Larger models require more VRAM/RAM. Ensure your system has sufficient resources."""
    },
    
    "output_formats": {
        "title": "Output Formats",
        "content": """Choose one or more formats for your transcription output.

Available formats:
‚Ä¢ SRT: Standard subtitle format, widely compatible with video players and editing software. Includes timestamps and text.
‚Ä¢ VTT: Web Video Text Tracks format, similar to SRT, used for web videos. Includes timestamps and text.
‚Ä¢ TXT: Plain text file with transcription. Always includes timestamps along with the text content. Timestamps appear at the beginning of each segment.
‚Ä¢ JSON: Detailed data format with all metadata, timestamps, word-level data, and speaker information (if diarization enabled). Structured format for programmatic use.

Note: If diarization is enabled, speaker labels will be included in all formats except JSON."""
    },
    
    "presets": {
        "title": "Presets - Quick Start Configurations",
        "content": """Presets provide optimized starting configurations for different use cases. You can modify any settings after selecting a preset.

AVAILABLE PRESETS:

‚≠ê STANDARD (Recommended for most users)
‚Ä¢ Best for: General transcription tasks, interviews, podcasts, meetings
‚Ä¢ Model: large-v2 (excellent accuracy, reliable, fewer hallucinations)
‚Ä¢ Settings: Optimized for MAXIMUM accuracy in controlled interview environments
‚Ä¢ VAD: Enabled with pyannote_v3 (most accurate method)
‚Ä¢ Beam Size: 10 (maximum accuracy)
‚Ä¢ Patience: 5.0 (maximum completeness)
‚Ä¢ Word Timestamps: Enabled
‚Ä¢ Audio Filters: Loudness normalization only
‚Ä¢ Output: TXT format
‚Ä¢ When to use: Most common use case - clean audio, interviews, general transcription needs

‚ö° TURBO (Speed optimized)
‚Ä¢ Best for: Quick transcriptions when speed is priority
‚Ä¢ Model: turbo (fast while maintaining good accuracy)
‚Ä¢ Settings: Balanced for speed and accuracy
‚Ä¢ VAD: Enabled with silero_v4_fw (faster method)
‚Ä¢ Beam Size: 5 (default, balanced)
‚Ä¢ Patience: 2.0 (default, faster)
‚Ä¢ Word Timestamps: Enabled
‚Ä¢ Audio Filters: Light normalization
‚Ä¢ Output: TXT format
‚Ä¢ When to use: When you need fast results and can accept slightly lower accuracy

üé§ DIARIZE (Speaker identification)
‚Ä¢ Best for: Interviews, meetings, podcasts with multiple speakers
‚Ä¢ Model: large-v2 (best for speaker identification accuracy)
‚Ä¢ Settings: Optimized for MAXIMUM accuracy in controlled interview environments
‚Ä¢ Diarization: Enabled with pyannote_v3.1 (latest, most accurate method)
‚Ä¢ VAD: Enabled with pyannote_v3 (most accurate)
‚Ä¢ Beam Size: 10 (maximum accuracy)
‚Ä¢ Patience: 5.0 (maximum completeness)
‚Ä¢ Word Timestamps: Enabled
‚Ä¢ Audio Filters: Loudness normalization
‚Ä¢ Output: TXT, SRT, VTT formats (multiple formats for speaker labeling)
‚Ä¢ When to use: When you need to identify who is speaking (interviews, meetings, multi-speaker content)
‚Ä¢ ‚ö†Ô∏è IMPORTANT: Set the exact number of speakers if known (dramatically improves accuracy)

üìû PHONE CONVERSATION AUDIO (Low-quality/noisy audio)
‚Ä¢ Best for: Phone calls, low-quality recordings, noisy audio
‚Ä¢ Model: large-v2 (robust for degraded audio)
‚Ä¢ Settings: Optimized for challenging audio conditions
‚Ä¢ Diarization: Enabled with pyannote_v3.1
‚Ä¢ VAD: Enabled with pyannote_v3 (most accurate for noisy audio)
‚Ä¢ Beam Size: 8 (higher for noisy conditions)
‚Ä¢ Patience: 4.0 (better for degraded audio)
‚Ä¢ Word Timestamps: Enabled
‚Ä¢ Audio Filters: Aggressive preprocessing
  - Speech normalization (amplifies quiet speech)
  - Loudness normalization
  - Low/high pass filter (removes non-speech frequencies)
  - Denoise: 15 (moderate noise reduction)
‚Ä¢ Output: TXT, SRT, VTT formats
‚Ä¢ When to use: Phone calls, recordings with background noise, low-quality audio
‚Ä¢ ‚ö†Ô∏è IMPORTANT: For phone calls, set speaker count to 2 if known (most calls are 2 speakers)

üîß CUSTOM
‚Ä¢ Best for: Advanced users who want full control
‚Ä¢ Settings: No pre-configured settings - you configure everything manually
‚Ä¢ When to use: When you want to set all options yourself from scratch

TIPS:
‚Ä¢ You can modify any setting after selecting a preset
‚Ä¢ For best diarization accuracy, always set the exact speaker count if known
‚Ä¢ Presets are starting points - feel free to customize based on your specific needs"""
    },
    
    "audio_filters": {
        "title": "Audio Filters",
        "content": """Audio filters preprocess your audio to improve transcription quality.

‚Ä¢ Speech Normalization (ff_speechnorm): Amplifies quiet speech to make it more audible. Use when speakers are too quiet.
‚Ä¢ Loudness Normalization (ff_loudnorm): Normalizes audio volume to EBU R128 broadcast standard. Use for consistent volume levels.
‚Ä¢ Low/High Pass Filter (ff_lowhighpass): Removes frequencies outside 50Hz-7800Hz range, focusing on speech frequencies. Use to reduce background noise.
‚Ä¢ Tempo Adjustment (ff_tempo): Adjusts playback speed. Values below 1.0 slow down (helpful for fast speech), above 1.0 speed up.
‚Ä¢ Denoise (ff_fftdn): Reduces background noise using Fast Fourier Transform. Higher values = more aggressive (0-97, default: 0 = disabled).

Use filters when your audio has quality issues. Start with one filter at a time to see the effect."""
    },
    
    "advanced_options": {
        "title": "Advanced Transcription Options",
        "content": """These options fine-tune the transcription process.

‚Ä¢ Temperature: Controls randomness in transcription. 0 = deterministic (recommended for consistent results), higher values = more variation. Default: 0.
‚Ä¢ Beam Size: Number of transcription candidates to consider. Higher = more accurate but slower. Range: 1-10, default: 5.
‚Ä¢ Patience: How long the model waits before finalizing a segment. Higher = more patient, may improve accuracy. Default: 2.0.
‚Ä¢ Best Of: Number of candidates when sampling with non-zero temperature. Default: 5.
‚Ä¢ Length Penalty: Token length penalty coefficient. Default: 1.0.
‚Ä¢ Repetition Penalty: Penalty for repeating tokens. Values > 1.0 penalize repetition. Default: 1.0.

Most users should leave these at default values unless experiencing specific issues."""
    },
    
    "diarization_settings": {
        "title": "Speaker Count Settings - Accuracy Optimization",
        "content": """Configure how many speakers to identify in your audio. Setting these correctly is CRITICAL for accuracy.

‚Ä¢ Number of Speakers: ‚≠ê MOST IMPORTANT - Set the exact number of speakers if known. This dramatically improves accuracy. The program will look for exactly this many speakers, which prevents false positives and improves separation quality.

‚Ä¢ Min Speakers: Set minimum number of speakers to detect. Use when you know there are at least X speakers but not the exact count.

‚Ä¢ Max Speakers: Set maximum number of speakers to detect. Use when you know there are at most X speakers but not the exact count.

ACCURACY BEST PRACTICES:
1. ‚≠ê SET EXACT COUNT: If you know there are exactly 2, 3, 4, etc. speakers, set "Number of Speakers" to that exact number. This is the SINGLE MOST IMPORTANT setting for accuracy.

2. Use Min/Max when you have a range: If you know there are 2-4 speakers but not the exact count, set Min=2, Max=4.

3. Auto-detection (all empty): Only use when you truly don't know the speaker count. Less accurate than setting the exact count.

Examples:
‚Ä¢ Interview with 2 people: Set Number of Speakers = 2 ‚≠ê BEST ACCURACY
‚Ä¢ Meeting with 3-5 participants: Set Min = 3, Max = 5
‚Ä¢ Podcast with 2 hosts: Set Number of Speakers = 2 ‚≠ê BEST ACCURACY
‚Ä¢ Unknown number: Leave all empty (less accurate)

Remember: Setting the exact speaker count is the #1 way to improve diarization accuracy!"""
    },
    
    "device_selection": {
        "title": "Device Selection (CPU vs GPU)",
        "content": """Choose which device to use for processing.

‚Ä¢ Auto: Automatically detects and uses CUDA/GPU if available, otherwise uses CPU.
‚Ä¢ CUDA: Use GPU for processing. Much faster than CPU, especially for large models and diarization. Requires NVIDIA GPU with CUDA support.
‚Ä¢ CPU: Use CPU only. Slower but works on any computer.

Recommendation: Use Auto or CUDA if you have an NVIDIA GPU. GPU processing can be 5-10x faster than CPU."""
    },
    
    "diarize_device": {
        "title": "Diarization Device Selection",
        "content": """Choose which device to use specifically for speaker diarization processing.

‚Ä¢ Auto: Automatically detects and uses CUDA/GPU if available, otherwise uses CPU. Recommended default.

‚Ä¢ CUDA: Use GPU for diarization. MUCH faster than CPU (5-10x speedup). Strongly recommended if you have an NVIDIA GPU with CUDA support. Diarization is computationally intensive, so GPU makes a huge difference.

‚Ä¢ CPU: Use CPU only for diarization. Works on any computer but significantly slower. Only use if you don't have a compatible GPU.

IMPORTANT FOR ACCURACY:
‚Ä¢ Device selection does NOT affect accuracy - GPU and CPU produce the same results
‚Ä¢ GPU is much faster, making it practical to use larger models and higher settings
‚Ä¢ For best accuracy workflow: Use GPU + larger models (large-v2, large-v3-turbo) + higher beam size

Note: This is separate from the main transcription device setting. You can use GPU for diarization even if using CPU for transcription (though not recommended).

Recommendation: Use Auto or CUDA if available. GPU makes diarization practical for real-world use."""
    },
    
    "task": {
        "title": "Task Selection (Transcribe vs Translate)",
        "content": """Choose what the program should do with your audio.

‚Ä¢ Transcribe: Converts speech to text in the original language. The output will be in whatever language is spoken in the audio (e.g., Spanish audio ‚Üí Spanish text).

‚Ä¢ Translate: Converts speech to text and translates it to English. The output will always be in English, regardless of the source language.

When to use each:
‚Ä¢ Use Transcribe when you want to preserve the original language or when working with English audio.
‚Ä¢ Use Translate when you need English text from non-English audio.

Note: When using Translate, you should still select the source language (the language spoken in the audio) for best accuracy."""
    },
    
    "language": {
        "title": "Language Selection",
        "content": """Select the language spoken in your audio file.

‚Ä¢ Auto-detect: Let the program automatically identify the language. Works well for most cases but may be slightly less accurate than manual selection.

‚Ä¢ Specific Language: Select the exact language for better accuracy. This is especially important for:
  - Audio with multiple languages
  - Languages that are similar to each other
  - When using the Translate task

Available languages include: English, Spanish, French, German, Japanese, Chinese, and many more.

Tip: If you're unsure, start with Auto-detect. If the results are poor, try selecting the language manually."""
    },
    
    "speaker_label": {
        "title": "Speaker Label",
        "content": """Customize the prefix used to identify speakers in the output.

Default: "SPEAKER"

Examples:
‚Ä¢ "SPEAKER" ‚Üí SPEAKER_00, SPEAKER_01, etc.
‚Ä¢ "Person" ‚Üí Person_00, Person_01, etc.
‚Ä¢ "Host" ‚Üí Host_00, Host_01, etc. (good for podcasts)
‚Ä¢ "Interviewer" ‚Üí Interviewer_00, Interviewee_01, etc.

This label appears in the transcription output to identify which speaker said each line. You can customize it to match your use case or workflow."""
    },
    
    "diarize_threads": {
        "title": "Diarization Threads",
        "content": """Number of CPU threads to use for diarization processing.

‚Ä¢ Auto (0): Automatically determines the optimal number of threads based on your CPU. Recommended for most users.

‚Ä¢ Manual: Set a specific number of threads (1-32). Use this if:
  - You want to limit CPU usage
  - You're running other intensive programs
  - Auto-detection isn't working well

Note: This only affects CPU processing. If you're using GPU/CUDA, this setting has no effect.

For best accuracy: Use Auto or set to match your CPU core count."""
    },
    
    "diarize_ff": {
        "title": "Diarize After Filters",
        "content": """Control the order of processing: when diarization runs relative to audio filters.

What it does:
This checkbox determines whether diarization is applied BEFORE or AFTER audio filters (like denoise, normalization, etc.).

‚Ä¢ CHECKED (Enabled - Recommended): 
  Processing order: Audio Filters ‚Üí Diarization ‚Üí Transcription
  - Audio is cleaned/filtered first (noise reduction, normalization, etc.)
  - Then diarization runs on the cleaned audio
  - Better accuracy because diarization works with cleaner, better-quality audio
  - Recommended when using audio filters

‚Ä¢ UNCHECKED (Disabled):
  Processing order: Diarization ‚Üí Audio Filters ‚Üí Transcription
  - Diarization runs on original, unfiltered audio
  - Then filters are applied to the audio
  - May be slightly faster but potentially less accurate
  - Use only if you're not using audio filters

When to use each:
‚Ä¢ CHECKED: Use when you have audio filters enabled (denoise, normalization, etc.) - this is the default and recommended setting
‚Ä¢ UNCHECKED: Only use if you're not using any audio filters and want slightly faster processing

Recommendation: Keep this CHECKED (enabled) for best accuracy, especially when using audio filters."""
    },
    
    "diarize_only": {
        "title": "Diarize Only (No Transcription)",
        "content": """Perform speaker identification WITHOUT transcribing the audio.

What it does:
‚Ä¢ Identifies and labels different speakers in the audio
‚Ä¢ Creates speaker segments with timestamps
‚Ä¢ Does NOT create any transcription text
‚Ä¢ Outputs only speaker identification data

When to use:
‚Ä¢ You only need to know WHO is speaking and WHEN, not WHAT they're saying
‚Ä¢ You want to analyze speaker patterns or conversation flow
‚Ä¢ You'll do transcription separately or with different settings
‚Ä¢ Testing or debugging diarization settings

Output:
‚Ä¢ Speaker segments with timestamps
‚Ä¢ Speaker labels (SPEAKER_00, SPEAKER_01, etc.)
‚Ä¢ No transcription text

Note: This is useful for specialized workflows. Most users will want transcription AND diarization together (leave this unchecked)."""
    },
    
    "return_embeddings": {
        "title": "Return Embeddings",
        "content": """Save speaker embedding vectors to separate files.

What are embeddings?
Speaker embeddings are mathematical representations of each speaker's voice characteristics. They're used internally by the diarization system to identify and distinguish speakers.

What this option does:
‚Ä¢ Saves one embedding file per speaker
‚Ä¢ Creates separate files for each unique speaker identified
‚Ä¢ Files contain the voice "fingerprint" data for each speaker

When to use:
‚Ä¢ Advanced analysis of speaker characteristics
‚Ä¢ Research or development work
‚Ä¢ Building speaker recognition systems
‚Ä¢ Debugging diarization issues
‚Ä¢ Creating speaker profiles for future matching

File format:
‚Ä¢ One file per speaker (e.g., speaker_0.emb, speaker_1.emb)
‚Ä¢ Binary or text format depending on implementation
‚Ä¢ Contains numerical vectors representing voice characteristics

Note: This is an advanced feature. Most users don't need this unless doing specialized work with speaker recognition."""
    },
    
    "diarize_dump": {
        "title": "Dump Diarization Output",
        "content": """Save detailed diarization debug information to files.

What it does:
‚Ä¢ Saves intermediate diarization processing data
‚Ä¢ Creates debug files showing how speakers were identified
‚Ä¢ Includes timing information, confidence scores, and processing details
‚Ä¢ Useful for troubleshooting diarization issues

Output files:
‚Ä¢ Diarization timing data
‚Ä¢ Speaker segment boundaries
‚Ä¢ Confidence scores for speaker identification
‚Ä¢ Processing logs and intermediate results

When to use:
‚Ä¢ Troubleshooting diarization accuracy issues
‚Ä¢ Understanding why speakers were identified incorrectly
‚Ä¢ Debugging diarization performance problems
‚Ä¢ Research or development work
‚Ä¢ Fine-tuning diarization settings

What you'll get:
‚Ä¢ Detailed logs of the diarization process
‚Ä¢ Information about how the system made decisions
‚Ä¢ Data that can help identify why certain speakers were or weren't detected

Note: This creates additional files and increases processing time slightly. Only enable when you need to debug or analyze the diarization process."""
    },
    
    "ff_speechnorm": {
        "title": "Speech Normalization",
        "content": """Amplify quiet speech segments to make them more audible and easier to transcribe.

What it does:
‚Ä¢ Automatically detects quiet speech segments
‚Ä¢ Amplifies (increases volume of) quiet speech
‚Ä¢ Makes soft-spoken words more audible
‚Ä¢ Improves transcription accuracy for quiet speakers

When to use:
‚Ä¢ Audio with speakers who talk quietly
‚Ä¢ Recordings with inconsistent volume levels
‚Ä¢ Interviews where one person is much quieter than another
‚Ä¢ Audio where speech volume varies significantly
‚Ä¢ When transcription misses quiet words or phrases

How it works:
‚Ä¢ Analyzes audio to identify speech segments
‚Ä¢ Detects segments that are quieter than average
‚Ä¢ Amplifies those segments to match normal speech levels
‚Ä¢ Preserves loud segments at their original level

Benefits:
‚Ä¢ Better transcription of quiet speech
‚Ä¢ More consistent volume levels
‚Ä¢ Improved accuracy for soft-spoken speakers
‚Ä¢ Better handling of volume variations

Note: This filter focuses specifically on speech frequencies. It's different from general loudness normalization which affects the entire audio signal.

Recommendation: Enable if you have issues with quiet speech not being transcribed accurately."""
    },
    
    "ff_loudnorm": {
        "title": "Loudness Normalization (EBU R128)",
        "content": """Normalize audio volume to broadcast industry standard (EBU R128 loudness standard).

What it does:
‚Ä¢ Adjusts overall audio volume to meet broadcast standards
‚Ä¢ Ensures consistent loudness across the entire audio file
‚Ä¢ Applies professional broadcast audio normalization
‚Ä¢ Makes audio suitable for professional use

EBU R128 Standard:
‚Ä¢ European Broadcasting Union standard for audio loudness
‚Ä¢ Used by TV, radio, and streaming services
‚Ä¢ Ensures consistent volume across different content
‚Ä¢ Prevents audio from being too loud or too quiet

When to use:
‚Ä¢ Preparing audio for broadcast or streaming
‚Ä¢ Audio with inconsistent volume levels
‚Ä¢ Professional transcription projects
‚Ä¢ When you need consistent loudness
‚Ä¢ Audio that will be used in professional contexts

How it works:
‚Ä¢ Measures integrated loudness (overall perceived volume)
‚Ä¢ Adjusts gain to target loudness level (-23 LUFS standard)
‚Ä¢ Applies true peak limiting to prevent clipping
‚Ä¢ Maintains audio quality while normalizing volume

Benefits:
‚Ä¢ Professional-grade audio normalization
‚Ä¢ Consistent volume levels
‚Ä¢ Meets broadcast standards
‚Ä¢ Better for professional use cases
‚Ä¢ Prevents audio from being too loud or too quiet

Note: This is more comprehensive than speech normalization - it normalizes the entire audio signal, not just speech segments.

Recommendation: Use for professional projects or when you need broadcast-quality audio normalization."""
    },
    
    "ff_lowhighpass": {
        "title": "Low/High Pass Filter",
        "content": """Remove frequencies outside the speech range (50Hz to 7800Hz) to focus on human speech.

What it does:
‚Ä¢ Removes very low frequencies (below 50Hz) - rumble, wind noise, etc.
‚Ä¢ Removes very high frequencies (above 7800Hz) - hiss, cymbals, etc.
‚Ä¢ Keeps frequencies in the human speech range
‚Ä¢ Focuses audio processing on speech content

Frequency ranges:
‚Ä¢ Low frequencies (removed): Below 50Hz - rumble, wind, traffic noise
‚Ä¢ Speech frequencies (kept): 50Hz to 7800Hz - human voice range
‚Ä¢ High frequencies (removed): Above 7800Hz - hiss, cymbals, high-pitched noise

When to use:
‚Ä¢ Audio with low-frequency rumble or wind noise
‚Ä¢ Recordings with high-frequency hiss or static
‚Ä¢ Audio with background noise outside speech range
‚Ä¢ When you want to focus processing on speech only
‚Ä¢ Recordings made in noisy environments

How it works:
‚Ä¢ Uses sinc filter for low frequencies (high quality)
‚Ä¢ Uses afir filter for high frequencies
‚Ä¢ Preserves speech frequencies (50Hz-7800Hz)
‚Ä¢ Removes frequencies that don't contain speech

Benefits:
‚Ä¢ Reduces non-speech noise
‚Ä¢ Improves transcription accuracy
‚Ä¢ Cleaner audio for processing
‚Ä¢ Better focus on speech content
‚Ä¢ Reduces interference from background noise

Examples of what gets removed:
‚Ä¢ Low frequencies: Rumble, wind, traffic, machinery
‚Ä¢ High frequencies: Hiss, static, cymbals, electronic noise

Note: This is a frequency filter, not a noise reduction filter. It removes entire frequency ranges, which can affect music or other non-speech audio.

Recommendation: Enable when you have low-frequency rumble or high-frequency hiss that interferes with transcription."""
    },
    
    "compute_type": {
        "title": "Compute Type (Quantization)",
        "content": """Control how numbers are stored and processed in the AI model. Affects speed, memory usage, and accuracy.

Available options:
‚Ä¢ auto: Automatically selects the best type based on your hardware. Recommended for most users. Best balance of speed and accuracy.

‚Ä¢ default: Uses the model's default precision. Good general-purpose option.

‚Ä¢ int8: 8-bit integers. Fastest, uses least memory, but slightly less accurate. Good for speed when accuracy is less critical.

‚Ä¢ float16: 16-bit floating point. Fast, uses less memory than float32, good accuracy. Good balance for GPU processing.

‚Ä¢ float32: 32-bit floating point. Most accurate, uses most memory, slower. Best accuracy but requires more resources.

What is quantization?
Quantization reduces the precision of numbers in the AI model to make it run faster and use less memory. Lower precision = faster but potentially less accurate.

Speed vs Accuracy:
‚Ä¢ int8: Fastest ‚ö°, least accurate
‚Ä¢ float16: Fast üöÄ, good accuracy
‚Ä¢ float32: Slower üê¢, most accurate ‚≠ê
‚Ä¢ auto: Balanced (recommended)

When to use each:
‚Ä¢ auto: Use for most cases - let the system choose
‚Ä¢ int8: When speed is critical and slight accuracy loss is acceptable
‚Ä¢ float16: Good for GPU processing, fast with good accuracy
‚Ä¢ float32: When maximum accuracy is required and you have sufficient resources

Memory usage:
‚Ä¢ int8: Uses least memory (good for limited VRAM)
‚Ä¢ float16: Moderate memory usage
‚Ä¢ float32: Uses most memory (requires more VRAM/RAM)

Recommendation: Use 'auto' for best results. The system will choose the optimal type based on your hardware and model."""
    },
    
    "standard_preset": {
        "title": "Standard Preset",
        "content": """Apply optimized subtitle formatting settings for standard use cases.

What it does:
Automatically configures multiple subtitle formatting options:
‚Ä¢ Max line width: 42 characters per line
‚Ä¢ Max line count: 2 lines per subtitle segment
‚Ä¢ Max comma percentage: 70% (breaks lines at commas when line is 70% full)
‚Ä¢ Sentence mode: Enabled (splits subtitles at sentence boundaries)

Result:
‚Ä¢ Subtitles formatted for easy reading
‚Ä¢ Two-line maximum per subtitle
‚Ä¢ Appropriate line breaks
‚Ä¢ Sentence-based segmentation
‚Ä¢ Professional subtitle appearance

When to use:
‚Ä¢ Creating subtitles for videos
‚Ä¢ Standard subtitle formatting needs
‚Ä¢ When you want professional-looking subtitles
‚Ä¢ General-purpose transcription projects
‚Ä¢ Most common use cases

Benefits:
‚Ä¢ Consistent subtitle formatting
‚Ä¢ Easy to read on screen
‚Ä¢ Professional appearance
‚Ä¢ Optimized line breaks

Important Notes:
‚Ä¢ Word Timestamps Requirement: The faster-whisper-xxl.exe program REQUIRES --word_timestamps=True when using --standard. If you enable Standard Preset, Word Timestamps will be automatically enabled. You cannot use Standard Preset without Word Timestamps due to this program requirement.
‚Ä¢ Sentence-based segmentation
‚Ä¢ If you want to remove timestamps from the output after processing, use the "Remove Timestamps" feature that appears after transcription completes.
‚Ä¢ Incompatible with Custom Subtitle Formatting: If you customize any subtitle formatting options (Max Line Width, Max Line Count, Max Comma Percentage, or Sentence Mode), Standard Preset will be automatically disabled. Standard Preset uses fixed values and cannot be combined with custom formatting.

Recommendation: Enable for most subtitle creation projects. Provides good default formatting."""
    },
    
    "max_line_width": {
        "title": "Max Line Width",
        "content": """Set the maximum number of characters per line in subtitle output.

What it does:
‚Ä¢ Controls how many characters appear on each line of a subtitle
‚Ä¢ Prevents subtitles from being too long and hard to read
‚Ä¢ Automatically breaks lines when the limit is reached

Default values:
‚Ä¢ Standard Preset: 42 characters
‚Ä¢ Custom: You can set any value from 1 to 200

When to use:
‚Ä¢ Creating subtitles for videos
‚Ä¢ When you want to control subtitle line length
‚Ä¢ For different screen sizes or readability preferences
‚Ä¢ When Standard Preset doesn't meet your needs

Examples:
‚Ä¢ 42 characters: Standard for most video players (Standard Preset default)
‚Ä¢ 30-35 characters: Shorter lines, good for smaller screens
‚Ä¢ 50-60 characters: Longer lines, good for larger displays
‚Ä¢ 0 or Auto: Let the program decide automatically

Important Notes:
‚Ä¢ This option is disabled when Standard Preset is enabled
‚Ä¢ If you change this value, Standard Preset will be automatically disabled
‚Ä¢ Works with Max Line Count to control subtitle appearance
‚Ä¢ Only applies to subtitle formats (SRT, VTT), not plain text

Recommendation: Use 42 characters (Standard Preset default) for most cases, or customize based on your display needs."""
    },
    
    "max_line_count": {
        "title": "Max Line Count",
        "content": """Set the maximum number of lines per subtitle segment.

What it does:
‚Ä¢ Controls how many lines each subtitle can have
‚Ä¢ Prevents subtitles from taking up too much screen space
‚Ä¢ Ensures subtitles are readable and not overwhelming

Default values:
‚Ä¢ Standard Preset: 2 lines
‚Ä¢ Custom: You can set any value from 1 to 10

When to use:
‚Ä¢ Creating subtitles for videos
‚Ä¢ When you want to control how many lines appear at once
‚Ä¢ For different screen sizes or readability preferences
‚Ä¢ When Standard Preset doesn't meet your needs

Examples:
‚Ä¢ 2 lines: Standard for most video players (Standard Preset default)
‚Ä¢ 1 line: Single-line subtitles, minimal screen space
‚Ä¢ 3-4 lines: More text per subtitle, good for slower reading speeds
‚Ä¢ Higher values: More text visible, but may cover more of the video

Important Notes:
‚Ä¢ This option is disabled when Standard Preset is enabled
‚Ä¢ If you change this value, Standard Preset will be automatically disabled
‚Ä¢ Works with Max Line Width to control subtitle appearance
‚Ä¢ Only applies to subtitle formats (SRT, VTT), not plain text

Recommendation: Use 2 lines (Standard Preset default) for most cases, as it provides a good balance between readability and screen space."""
    },
    
    "max_comma_percentage": {
        "title": "Max Comma Percentage",
        "content": """Set when to break subtitle lines at commas.

What it does:
‚Ä¢ Controls line breaking behavior at commas
‚Ä¢ When a line reaches this percentage of Max Line Width, the program will prefer to break at commas
‚Ä¢ Helps create more natural line breaks in subtitles

Default values:
‚Ä¢ Standard Preset: 70%
‚Ä¢ Custom: You can choose from 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, or 100%

How it works:
‚Ä¢ If set to 70%: When a line reaches 70% of Max Line Width, the program will look for commas to break the line
‚Ä¢ Lower percentages: Break lines earlier, more aggressive line breaking
‚Ä¢ Higher percentages: Break lines later, allow longer lines before breaking
‚Ä¢ 100%: Only break at commas when the line is completely full

When to use:
‚Ä¢ Creating subtitles for videos
‚Ä¢ When you want to control where lines break
‚Ä¢ For more natural-looking subtitle formatting
‚Ä¢ When Standard Preset doesn't meet your needs

Examples:
‚Ä¢ 70%: Standard Preset default - good balance
‚Ä¢ 50-60%: More aggressive line breaking, shorter lines
‚Ä¢ 80-90%: Less aggressive, longer lines before breaking
‚Ä¢ 100%: Only break at commas when absolutely necessary

Important Notes:
‚Ä¢ This option is disabled when Standard Preset is enabled
‚Ä¢ If you change this value, Standard Preset will be automatically disabled
‚Ä¢ Works with Max Line Width and Max Line Count
‚Ä¢ Only applies to subtitle formats (SRT, VTT), not plain text

Recommendation: Use 70% (Standard Preset default) for most cases, as it provides natural line breaks."""
    },
    
    "sentence_mode": {
        "title": "Sentence Mode",
        "content": """Enable sentence-based subtitle segmentation.

What it does:
‚Ä¢ Splits subtitles at sentence boundaries (periods, exclamation marks, question marks)
‚Ä¢ Creates more natural subtitle breaks
‚Ä¢ Improves readability by keeping complete sentences together

Default behavior:
‚Ä¢ Automatically enabled when Diarization is enabled
‚Ä¢ Automatically enabled when Standard Preset is enabled
‚Ä¢ Can be manually enabled for custom formatting

When to use:
‚Ä¢ Creating subtitles for videos
‚Ä¢ When you want sentence-based segmentation
‚Ä¢ For more natural subtitle appearance
‚Ä¢ When Standard Preset doesn't meet your needs
‚Ä¢ When diarization is not enabled but you want sentence breaks

How it works:
‚Ä¢ The program identifies sentence boundaries in the transcription
‚Ä¢ Subtitles are created to align with these boundaries
‚Ä¢ Results in more readable, natural-looking subtitles

Important Notes:
‚Ä¢ This option is disabled when Standard Preset is enabled
‚Ä¢ This option is automatically enabled (and disabled) when Diarization is enabled
‚Ä¢ If you manually enable this when Standard Preset is enabled, Standard Preset will be disabled
‚Ä¢ Works with other subtitle formatting options (Max Line Width, Max Line Count, Max Comma Percentage)
‚Ä¢ Only applies to subtitle formats (SRT, VTT), not plain text

Recommendation: Enable for most subtitle projects. Provides more natural subtitle breaks and better readability."""
    },
    
    "batch_recursive": {
        "title": "Batch Recursive",
        "content": """Process multiple files and folders recursively (including subdirectories).

What it does:
‚Ä¢ Processes all files in the selected folder
‚Ä¢ Recursively processes files in all subdirectories
‚Ä¢ Handles multiple files in one operation
‚Ä¢ Automatically finds all audio/video files in folder structure

How it works:
‚Ä¢ You select a folder (or multiple files/folders)
‚Ä¢ Program scans the folder and all subfolders
‚Ä¢ Finds all supported audio/video files
‚Ä¢ Processes each file with the same settings
‚Ä¢ Saves outputs in appropriate locations

When to use:
‚Ä¢ Processing multiple files at once
‚Ä¢ Organizing files in folders and subfolders
‚Ä¢ Batch transcription projects
‚Ä¢ Processing entire directory structures
‚Ä¢ When you have many files to transcribe

Output behavior:
‚Ä¢ Output files are saved in the same location as input files (by default)
‚Ä¢ Or in the specified output directory
‚Ä¢ Maintains folder structure
‚Ä¢ Each file gets its own transcription output

Benefits:
‚Ä¢ Process many files without manual selection
‚Ä¢ Maintains folder organization
‚Ä¢ Efficient for large projects
‚Ä¢ Consistent settings across all files
‚Ä¢ Saves time on multiple files

Example:
If you select a folder with this structure:
  /Recordings/
    /Meeting1/
      audio1.mp3
      audio2.mp3
    /Meeting2/
      audio3.mp3

All three audio files will be processed automatically.

Note: Make sure you have enough disk space and processing time for all files. Large batches can take significant time.

Recommendation: Enable when processing multiple files or entire folder structures."""
    },
    
    "check_files": {
        "title": "Check Files",
        "content": """Verify input files for errors before processing begins.

What it does:
‚Ä¢ Checks each input file before processing
‚Ä¢ Verifies files are valid and readable
‚Ä¢ Detects corrupted or invalid files
‚Ä¢ Identifies files that can't be processed
‚Ä¢ Prevents processing errors by catching problems early

Checks performed:
‚Ä¢ File exists and is accessible
‚Ä¢ File format is valid
‚Ä¢ File is not corrupted
‚Ä¢ File can be opened and read
‚Ä¢ File is a supported audio/video format

When to use:
‚Ä¢ Processing multiple files (batch processing)
‚Ä¢ When you're unsure about file quality
‚Ä¢ Processing files from unknown sources
‚Ä¢ Large batch operations
‚Ä¢ When you want to avoid processing errors

Benefits:
‚Ä¢ Catches problems before processing starts
‚Ä¢ Saves time by identifying bad files early
‚Ä¢ Prevents wasted processing on corrupted files
‚Ä¢ Better error reporting
‚Ä¢ More reliable batch processing

What happens:
‚Ä¢ If a file is valid: Processing continues normally
‚Ä¢ If a file has errors: Error is reported, file is skipped, processing continues with other files
‚Ä¢ You get a report of which files were checked and any issues found

Error detection:
‚Ä¢ Corrupted files
‚Ä¢ Unsupported formats
‚Ä¢ Files that can't be read
‚Ä¢ Invalid file structures
‚Ä¢ Permission issues

Note: This adds a small amount of time before processing starts, but can save significant time by catching problems early.

Recommendation: Enable when processing multiple files or when file quality is uncertain. Especially useful with Batch Recursive."""
    },
    
    "vad_enable": {
        "title": "Voice Activity Detection (VAD)",
        "content": """Voice Activity Detection identifies parts of audio that contain speech and filters out silence and background noise.

Benefits:
‚Ä¢ Improves transcription accuracy by focusing on speech segments
‚Ä¢ Reduces processing time by skipping silent parts
‚Ä¢ Better handling of audio with long pauses or background noise

When to disable:
‚Ä¢ Very short audio clips (< 30 seconds)
‚Ä¢ Audio that is already pre-processed
‚Ä¢ When you need to preserve all audio segments (even silence)

Recommendation: Keep enabled for most use cases. It significantly improves accuracy and speed."""
    },
    
    "word_timestamps": {
        "title": "Word Timestamps",
        "content": """Include precise timestamps for each individual word in the transcription.

Benefits:
‚Ä¢ Enables word-level timing in subtitle files
‚Ä¢ Required for "Highlight Words" (karaoke) feature
‚Ä¢ More precise subtitle synchronization
‚Ä¢ Better for video editing and synchronization

When to disable:
‚Ä¢ You only need sentence-level timestamps
‚Ä¢ Processing very long files where word timestamps add significant time
‚Ä¢ Plain text output where timestamps aren't needed

Note: Word timestamps are required for the karaoke/highlight words feature. Disabling this will also disable highlight words.

Recommendation: Keep enabled for subtitle formats (SRT, VTT). You can disable for plain text output."""
    },
    
    "highlight_words": {
        "title": "Highlight Words (Karaoke Effect)",
        "content": """Underline each word as it's spoken in subtitle files, creating a karaoke-style effect.

How it works:
‚Ä¢ Each word is highlighted/underlined at the exact moment it's spoken
‚Ä¢ Creates a visual karaoke effect in video players
‚Ä¢ Works with SRT and VTT subtitle formats

Requirements:
‚Ä¢ Word Timestamps must be enabled
‚Ä¢ Sentence mode cannot be active (but note: diarization automatically enables sentence mode)

Use cases:
‚Ä¢ Music videos with lyrics
‚Ä¢ Educational videos
‚Ä¢ Language learning content
‚Ä¢ Any video where you want words highlighted as they're spoken

Note: This feature increases processing time slightly. Disable if you don't need the karaoke effect."""
    },
    
    "ff_tempo": {
        "title": "Tempo Adjustment",
        "content": """Adjust the playback speed of your audio before transcription.

Range: 0.5 to 2.0
‚Ä¢ 1.0 = Normal speed (default, disabled)
‚Ä¢ Below 1.0 = Slows down audio (e.g., 0.8 = 80% speed)
‚Ä¢ Above 1.0 = Speeds up audio (e.g., 1.2 = 120% speed)

When to use:
‚Ä¢ Slow down (0.5-0.9): Very fast speech that's hard to transcribe accurately
‚Ä¢ Speed up (1.1-2.0): Very slow speech to reduce processing time (not recommended for accuracy)

Important notes:
‚Ä¢ Slowing down can improve accuracy for fast speakers
‚Ä¢ Speeding up reduces accuracy and is not recommended
‚Ä¢ This affects the entire audio file
‚Ä¢ Processing time is affected by tempo changes

Recommendation: Use 1.0 (normal) for best results. Only adjust if you have specific issues with speech speed."""
    },
    
    "ff_fftdn": {
        "title": "Denoise Filter",
        "content": """Reduce background noise in your audio using Fast Fourier Transform.

Range: 0 to 97
‚Ä¢ 0 = Disabled (default)
‚Ä¢ 12 = Normal strength (good starting point)
‚Ä¢ 30-50 = Moderate noise reduction
‚Ä¢ 70-97 = Aggressive noise reduction

When to use:
‚Ä¢ Audio with constant background noise (fans, air conditioning, etc.)
‚Ä¢ Recordings with hiss or static
‚Ä¢ Audio with low-level background chatter
‚Ä¢ Poor quality recordings

How it works:
‚Ä¢ Analyzes audio frequencies
‚Ä¢ Identifies and reduces noise patterns
‚Ä¢ Preserves speech frequencies

Tips:
‚Ä¢ Start with 12-20 for mild noise
‚Ä¢ Increase gradually if needed
‚Ä¢ Too high values (80+) may affect speech quality
‚Ä¢ Test on a short clip first

Warning: Very high values may introduce artifacts or affect speech clarity. Use conservatively."""
    },
    
    "temperature": {
        "title": "Temperature",
        "content": """Controls the randomness and creativity of the transcription.

Range: 0.0 to 1.0
‚Ä¢ 0.0 = Deterministic, most consistent (recommended)
‚Ä¢ 0.2-0.4 = Slight variation
‚Ä¢ 0.5-1.0 = More variation, less consistent

How it works:
‚Ä¢ Lower values = More predictable, consistent transcriptions
‚Ä¢ Higher values = More variation, may handle unusual words better but less consistent

When to adjust:
‚Ä¢ Keep at 0.0 for most cases (best consistency)
‚Ä¢ Increase slightly (0.2-0.3) if transcription is too conservative
‚Ä¢ Higher values may help with unusual accents or words, but reduce consistency

Recommendation: Keep at 0.0 (default) for best accuracy and consistency. Only adjust if you're experiencing specific issues."""
    },
    
    "beam_size": {
        "title": "Beam Size",
        "content": """Number of transcription candidates the model considers before choosing the best one.

Range: 1 to 10
‚Ä¢ Default: 5
‚Ä¢ Lower (1-3) = Faster, less accurate
‚Ä¢ Higher (6-10) = Slower, more accurate

How it works:
‚Ä¢ The model generates multiple possible transcriptions
‚Ä¢ Beam size determines how many candidates to evaluate
‚Ä¢ Higher beam size = more thorough search = better accuracy

When to adjust:
‚Ä¢ Increase (6-10) for maximum accuracy (slower processing)
‚Ä¢ Decrease (3-4) for faster processing (slight accuracy trade-off)
‚Ä¢ Keep at 5 for balanced speed and accuracy

For best accuracy: Use 7-10, especially for:
‚Ä¢ Important transcriptions
‚Ä¢ Audio with accents or unclear speech
‚Ä¢ When accuracy is more important than speed

Recommendation: Keep at 5 for most cases. Increase to 7-10 for maximum accuracy."""
    },
    
    "patience": {
        "title": "Patience",
        "content": """How long the model waits before finalizing a transcription segment.

Range: 0.0 to 10.0
‚Ä¢ Default: 2.0
‚Ä¢ Lower = Faster decisions, may cut off sentences
‚Ä¢ Higher = More patient, waits longer for complete sentences

How it works:
‚Ä¢ Controls when the model decides a segment is complete
‚Ä¢ Higher patience = model waits longer for sentence endings
‚Ä¢ Helps prevent cutting off sentences mid-thought

When to adjust:
‚Ä¢ Increase (3.0-5.0) for:
  - Long, complex sentences
  - Speakers who pause frequently
  - Better sentence completeness
‚Ä¢ Decrease (1.0-1.5) for:
  - Faster processing
  - Short, simple sentences
  - When speed is priority

For best accuracy: Use 3.0-4.0 for:
‚Ä¢ Complex content
‚Ä¢ Multiple speakers
‚Ä¢ Important transcriptions

Recommendation: Keep at 2.0 for most cases. Increase to 3.0-4.0 for better sentence completeness."""
    },
    
    "diarize_enable": {
        "title": "Speaker Diarization",
        "content": """Identify and label different speakers in your audio.

What it does:
‚Ä¢ Automatically detects when different people are speaking
‚Ä¢ Labels each speaker (SPEAKER_00, SPEAKER_01, etc.)
‚Ä¢ Creates separate transcription lines for each speaker

Perfect for:
‚Ä¢ Interviews (interviewer and interviewee)
‚Ä¢ Meetings with multiple participants
‚Ä¢ Podcasts with multiple hosts
‚Ä¢ Conversations and discussions
‚Ä¢ Any audio with 2+ speakers

How it works:
‚Ä¢ Uses advanced AI models to analyze voice characteristics
‚Ä¢ Identifies unique speakers based on voice patterns
‚Ä¢ Separates speech segments by speaker

Accuracy tips:
‚Ä¢ Use larger models (large-v2, large-v3-turbo) for better results
‚Ä¢ Set exact speaker count if known (significantly improves accuracy)
‚Ä¢ Use GPU/CUDA for faster processing
‚Ä¢ Enable audio filters to clean audio first (improves diarization accuracy)

Note: Diarization automatically enables sentence mode for better speaker separation."""
    }
}

def get_tooltip(key):
    """Get tooltip text for a given option key."""
    return TOOLTIPS.get(key, "")

def get_detailed_help(key):
    """Get detailed help content for a given option key."""
    return DETAILED_HELP.get(key, {"title": "Help", "content": "No help available for this option."})

